{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworksOptimizers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXqUGQUIjPrG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqbvDIUMmwrH"
      },
      "source": [
        "import torch\n",
        "class GlobalManager:\n",
        "    def __init__(self, use_gpu=False, log_state=False):\n",
        "        self.use_gpu = use_gpu\n",
        "        self.log_state = log_state\n",
        "        self.cpu_d = torch.device(\"cpu\")\n",
        "        self.device = torch.device(\"cuda\") if use_gpu else self.cpu\n",
        "        pass\n",
        "\n",
        "    def is_gpu(self):\n",
        "        return self.use_gpu\n",
        "    \n",
        "    def current(self):\n",
        "        return self.device\n",
        "\n",
        "    def cpu(self):\n",
        "        return self.cpu_d\n",
        "    \n",
        "    def log(self, msg):\n",
        "        if self.log_state:\n",
        "            print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aITu7f3Tm3Cp"
      },
      "source": [
        "class ExecutionEvents:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def set_model(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def on_epoch_start(self, epoch, **kwargs):\n",
        "        if hasattr(self, 'es'):\n",
        "            self.es(epoch, **kwargs)\n",
        "        pass\n",
        "    \n",
        "    def on_epoch_end(self, epoch, losses, avg_loss, **kwargs):\n",
        "        if hasattr(self, 'ee'):\n",
        "            self.ee(epoch, losses, avg_loss, **kwargs)\n",
        "        pass\n",
        "\n",
        "    def on_batch_processed(self, epoch, batch, loss, **kwargs):\n",
        "        if hasattr(self, 'bp'):\n",
        "            self.bp(epoch, batch, loss, **kwargs)\n",
        "        pass\n",
        "\n",
        "    def on_validated(self, epoch, loss, **kwargs):\n",
        "        if hasattr(self, 'v'):\n",
        "            self.v(epoch, loss, **kwargs)\n",
        "        pass\n",
        "\n",
        "    def attach_epoch_start(self, func):\n",
        "        self.es = func\n",
        "        pass\n",
        "    \n",
        "    def attach_epoch_end(self, func):\n",
        "        self.ee = func\n",
        "        pass\n",
        "\n",
        "    def attach_batch_processed(self, func):\n",
        "        self.bp = func\n",
        "        pass\n",
        "\n",
        "    def attach_validated(self, func):\n",
        "        self.v = func\n",
        "        pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ6GZOHnm3kv"
      },
      "source": [
        "class PreprocessingDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4d0Kopg5CHZ"
      },
      "source": [
        "class EarlyStopping(object):\n",
        "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        if torch.isnan(metrics):\n",
        "            return True\n",
        "\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (\n",
        "                            best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + (\n",
        "                            best * min_delta / 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geD1M5t0m4v0"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "class Executor:\n",
        "    def __init__(self, model, manager):\n",
        "        self.model = model.to(manager.current())\n",
        "        self.manager = manager\n",
        "    \n",
        "    def configure(self, optimizer, loss=\"cross_entropy\", preprocessor=None):\n",
        "        self.optimizer = optimizer\n",
        "        if isinstance(loss, str):\n",
        "            if loss == \"mse\":\n",
        "                self.criterion = F.mse_loss\n",
        "            elif loss == \"cross_entropy\":\n",
        "                self.criterion = F.cross_entropy\n",
        "            else:\n",
        "                raise Exception(\"Undefined loss func, use function or nn.Module instead!\")\n",
        "        elif isinstance(loss, nn.Module) or type(loss).__name__ == \"function\":\n",
        "            self.criterion = loss\n",
        "        else:\n",
        "            raise Exception(\"loss parameter must be string, function or nn.Module!\")\n",
        "        \n",
        "        if preprocessor is None:\n",
        "            self.preproc = False\n",
        "            pass\n",
        "        elif type(preprocessor).__name__ == \"function\":\n",
        "            self.preproc = True\n",
        "            self.preprocessor = preprocessor\n",
        "        else:\n",
        "            raise Exception(\"Preprocessor type not recognized!\")\n",
        "\n",
        "    def train(self, train_dataset, epochs, val_split=0.15, train_batch=32, val_batch=32,\n",
        "              event_listener=None, progress=True, metrics={},\n",
        "              early_stopping=False, patience=5, disturb=False):\n",
        "        total = len(train_dataset)\n",
        "        val_count = int(val_split * total)\n",
        "        self.train_set, self.val_set = random_split(train_dataset, (total - val_count, val_count))\n",
        "\n",
        "        train_loader = DataLoader(self.train_set, batch_size=train_batch, shuffle=True)\n",
        "        val_loader = DataLoader(self.val_set, batch_size=val_batch, shuffle=True)\n",
        "\n",
        "        if self.preproc:\n",
        "            train_loader = PreprocessingDataLoader(train_loader, self.preprocessor)\n",
        "            val_loader = PreprocessingDataLoader(val_loader, self.preprocessor)\n",
        "        event_listener = ExecutionEvents() if event_listener is None else event_listener\n",
        "        event_listener.set_model(self.model, self.optimizer)\n",
        "        if early_stopping:\n",
        "            self.es = EarlyStopping(patience=patience)\n",
        "        if disturb:\n",
        "            self.disturb = DisturbLabel(alpha=20, C=10)\n",
        "        for i in range(epochs):\n",
        "            #Training an epoch\n",
        "            model.train()\n",
        "            losses = {}\n",
        "            metrics_v = {}\n",
        "            for metric in metrics:\n",
        "                metrics_v[metric] = {}\n",
        "            it = enumerate(train_loader)\n",
        "            it = tqdm(it, total=len(train_loader), position=0) if progress else it\n",
        "            if progress:\n",
        "                it.set_description('Epoch %d' % i)\n",
        "            event_listener.on_epoch_start(i)\n",
        "            for batch_idx, (data, target) in it:\n",
        "                data, target = data.to(self.manager.current()), target.to(self.manager.current())\n",
        "                if disturb:\n",
        "                    target = self.disturb(target).to(self.manager.current())\n",
        "                def fwd():\n",
        "                    self.optimizer.zero_grad()\n",
        "                    output = self.model(data)\n",
        "                    loss = self.criterion(output, target)\n",
        "                    losses[batch_idx] = loss.item()\n",
        "                    for metric in metrics:\n",
        "                        metrics_v[metric][batch_idx] = metrics[metric](output, target).item()\n",
        "                    loss.backward(create_graph=True)\n",
        "                    return loss, output\n",
        "                self.optimizer.step(closure=fwd)\n",
        "                event_listener.on_batch_processed(i, batch_idx, losses[batch_idx])\n",
        "                if progress:\n",
        "                    m_mean = {k: np.mean(list(metrics_v[k].values())) for k in metrics}\n",
        "                    it.set_postfix(batch=batch_idx, loss=losses[batch_idx], mean_loss=np.mean(list(losses.values())),\n",
        "                                  **m_mean)\n",
        "            if progress:\n",
        "                it.close()\n",
        "            ls = list(losses.values())\n",
        "            m_mean = {k: np.mean(list(metrics_v[k].values())) for k in metrics}\n",
        "            m_all = {k+\"_all\": list(metrics_v[k].values()) for k in metrics}\n",
        "            event_listener.on_epoch_end(i, ls, np.mean(ls), **m_mean, **m_all)\n",
        "            #Validating an epoch\n",
        "            model.eval()\n",
        "\n",
        "            it = val_loader\n",
        "            it = tqdm(it, total=len(val_loader), position=0) if progress else it\n",
        "            if progress:\n",
        "                it.set_description('Validating Epoch %d' % i)\n",
        "\n",
        "            loss_val = []\n",
        "            metrics_val = {}\n",
        "            for metric in metrics:\n",
        "                metrics_val[metric] = []\n",
        "            with torch.no_grad():\n",
        "                for data, target in it:\n",
        "                    data, target = data.to(self.manager.current()), target.to(self.manager.current())\n",
        "                    output = self.model(data)\n",
        "                    loss = self.criterion(output, target)\n",
        "                    loss = loss.item()\n",
        "                    loss_val.append(loss)\n",
        "                    for metric in metrics:\n",
        "                        metrics_val[metric].append(metrics[metric](output, target).item())\n",
        "                    if progress:\n",
        "                        m_mean = {k: np.mean(metrics_val[k]) for k in metrics}\n",
        "                        it.set_postfix(loss=loss, mean_loss=np.mean(loss_val),\n",
        "                                    **m_mean)\n",
        "            m_mean = {k: np.mean(metrics_val[k]) for k in metrics}\n",
        "            m_all = {k+\"_all\": metrics_val[k] for k in metrics}\n",
        "            event_listener.on_validated(i, np.mean(loss_val), **m_mean, **m_all)\n",
        "            if early_stopping:\n",
        "                if self.es.step(torch.tensor(np.mean(loss_val))):\n",
        "                    break\n",
        "            if progress:\n",
        "                it.close()\n",
        "    def eval(self, test_dataset, batch_size=32, progress=True, metrics={}):\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        if self.preproc:\n",
        "            test_loader = PreprocessingDataLoader(test_loader, self.preprocessor)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        it = test_loader\n",
        "        it = tqdm(it, total=len(test_loader), position=0) if progress else it\n",
        "        if progress:\n",
        "            it.set_description(\"Evaluating\")\n",
        "\n",
        "        loss_val = []\n",
        "        metrics_val = {}\n",
        "        for metric in metrics:\n",
        "            metrics_val[metric] = []\n",
        "        outs = []\n",
        "        with torch.no_grad():\n",
        "            for data, target in it:\n",
        "                data, target = data.to(self.manager.current()), target.to(self.manager.current())\n",
        "                output = self.model(data)\n",
        "                outs.append(output)\n",
        "                loss = self.criterion(output, target).item()\n",
        "                loss_val.append(loss)\n",
        "                for metric in metrics:\n",
        "                    metrics_val[metric].append(metrics[metric](output, target).item())\n",
        "                if progress:\n",
        "                    m_mean = {k: np.mean(metrics_val[k]) for k in metrics}\n",
        "                    it.set_postfix(loss=loss, mean_loss=np.mean(loss_val),\n",
        "                                **m_mean)\n",
        "        m_mean = {k: np.mean(metrics_val[k]) for k in metrics}\n",
        "        m_mean[\"loss\"] = np.mean(loss_val)\n",
        "        if progress:\n",
        "            it.close()\n",
        "        return torch.cat(outs), m_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyb3qDbInWfw"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "def accuracy(pred, gt):\n",
        "    pred = pred.argmax(dim=1, keepdim=True)\n",
        "    correct = pred.eq(gt.view_as(pred)).sum()\n",
        "    return correct.float()/pred.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLlA3fRLr7RY"
      },
      "source": [
        "def loss(predicted, ground_truth):\n",
        "    return F.nll_loss(predicted, ground_truth)\n",
        "\n",
        "def loss_n(model, lambda_):\n",
        "    def loss(predicted, ground_truth):\n",
        "        return F.nll_loss(predicted, ground_truth)\n",
        "    return loss\n",
        "\n",
        "def loss_l1(model, lambda_):\n",
        "    def loss(predicted, ground_truth):\n",
        "        l1_reg = torch.tensor(0., requires_grad=True).cuda()\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                l1_reg += torch.norm(param, 1)\n",
        "        return F.nll_loss(predicted, ground_truth) + lambda_ * l1_reg\n",
        "    return loss\n",
        "\n",
        "def loss_l2(model, lambda_):\n",
        "    def loss(predicted, ground_truth):\n",
        "        l2_reg = torch.tensor(0., requires_grad=True).cuda()\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param)\n",
        "        return F.nll_loss(predicted, ground_truth) + lambda_ * l2_reg\n",
        "    return loss\n",
        "\n",
        "def loss_group_lasso(model, lambda_):\n",
        "    def loss(predicted, ground_truth):\n",
        "        gl_reg = torch.tensor(0., requires_grad=True).cuda()\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                gl_reg += param.norm(2, dim=1).sum()\n",
        "            if 'bias' in name:\n",
        "                gl_reg += param.norm(2)\n",
        "        return F.nll_loss(predicted, ground_truth) + lambda_ * gl_reg\n",
        "    return loss\n",
        "\n",
        "def loss_sparse_group_lasso(model, lambda_):\n",
        "    def loss(predicted, ground_truth):\n",
        "        gl_reg = torch.tensor(0., requires_grad=True).cuda()\n",
        "        l1_reg = torch.tensor(0., requires_grad=True).cuda()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                gl_reg += param.norm(2, dim=1).sum()\n",
        "                l1_reg += torch.norm(param, 1)               \n",
        "            if 'bias' in name:\n",
        "                gl_reg += param.norm(2)\n",
        "        return F.nll_loss(predicted, ground_truth) + lambda_ * (gl_reg + l1_reg)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG9PST6l3HIF"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "transformer = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                    ])\n",
        "aug_transformer = transforms.Compose([\n",
        "                        transforms.RandomCrop(32, padding=4),\n",
        "                        transforms.RandomGrayscale(0.3),\n",
        "                        transforms.RandomPerspective(),\n",
        "                        transforms.RandomRotation(degrees=90),\n",
        "                        transforms.RandomRotation(degrees=45),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                    ])\n",
        "\n",
        "cifar_train = CIFAR10('../data', train=True, download=True, transform=transformer)\n",
        "cifar_aug_train = CIFAR10('../data', train=True, download=True, transform=aug_transformer)\n",
        "cifar_aug_train, _ = random_split(cifar_aug_train, (15000, 35000))\n",
        "cifar_test = CIFAR10('../data', train=False, transform=transformer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt2gdCr25wwl"
      },
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[i] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(d) for d in self.datasets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAu__ZtI2oi0"
      },
      "source": [
        "class DisturbLabel(nn.Module):\n",
        "    def __init__(self, alpha, C):\n",
        "        super(DisturbLabel, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.C = C\n",
        "        # Multinoulli distribution\n",
        "        self.p_c = (1 - ((C - 1)/C) * (alpha/100))\n",
        "        self.p_i = (1 / C) * (alpha / 100)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # convert classes to index\n",
        "        y_tensor = y\n",
        "        y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "\n",
        "        # create disturbed labels\n",
        "        depth = self.C\n",
        "        y_one_hot = torch.ones(y_tensor.size()[0], depth) * self.p_i\n",
        "        y_one_hot.scatter_(1, y_tensor, self.p_c)\n",
        "        y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,)))\n",
        "\n",
        "        # sample from Multinoulli distribution\n",
        "        distribution = torch.distributions.OneHotCategorical(y_one_hot)\n",
        "        y_disturbed = distribution.sample()\n",
        "        y_disturbed = y_disturbed.max(dim=1)[1]  # back to categorical\n",
        "\n",
        "        return y_disturbed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGTIhTOVnXEr"
      },
      "source": [
        "class CifarNet(nn.Module):\n",
        "    def __init__(self, dropout=False):\n",
        "        super(CifarNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, 1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 128, 3, 1)\n",
        "        self.fc1 = nn.Linear(6*6*128, 720)\n",
        "        self.fc2 = nn.Linear(720, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.dropout1 = nn.Dropout2d(0.2)\n",
        "        self.dropout2 = nn.Dropout2d(0.1)\n",
        "        self.dropout3 = nn.Dropout2d(0.2)\n",
        "        self.dropout4 = nn.Dropout(0.15)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.dropout1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.dropout2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.dropout3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        if self.dropout:\n",
        "            x = self.dropout4(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fvIvuDCult_"
      },
      "source": [
        "import torch.optim as optim\n",
        "!wget -q https://github.com/LiyuanLucasLiu/RAdam/raw/master/radam/radam.py\n",
        "from radam import RAdam\n",
        "\n",
        "configList = {\n",
        "    \"radam-0.01\": lambda p: RAdam(p, lr=0.01, weight_decay=5e-4),\n",
        "    \"radam-0.001\": lambda p: RAdam(p, lr=0.001, weight_decay=5e-4),\n",
        "    \"adagrad-0.01\": lambda p: optim.Adagrad(p, lr=0.01),\n",
        "    \"adagrad-0.001\": lambda p: optim.Adagrad(p, lr=0.001),\n",
        "    \"sgd-0.01\": lambda p: optim.SGD(p, lr=0.01, momentum=0.9),\n",
        "    \"sgd-0.001\": lambda p: optim.SGD(p, lr=0.001, momentum=0.9),\n",
        "    \"adam-0.01\": lambda p: optim.Adam(p, lr=0.01),\n",
        "    \"adam-0.001\": lambda p: optim.Adam(p, lr=0.001),\n",
        "    \"adamax-0.01\": lambda p: optim.Adamax(p, lr=0.01),\n",
        "    \"adamax-0.001\": lambda p: optim.Adamax(p, lr=0.001),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8qp_e48wCJt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSv2kiz7ugJR"
      },
      "source": [
        "recording = []\n",
        "losses = [(\"normal\", loss_n)]\n",
        "for optK in configList:\n",
        "    for ln, l in losses:\n",
        "        data = {\n",
        "            \"loss_reg\": ln,\n",
        "            \"opt\": optK,\n",
        "            \"dropout\": True,\n",
        "            \"losses\": [],\n",
        "            \"val_losses\":[],\n",
        "            \"acc\":[],\n",
        "            \"val_acc\":[],\n",
        "        }\n",
        "\n",
        "        model = CifarNet(dropout=True)\n",
        "        train_set = cifar_train\n",
        "        executor = Executor(model, GlobalManager(use_gpu=True, log_state=True))\n",
        "        executor.configure(configList[optK](model.parameters()), l(model, 0), preprocessor=None)\n",
        "        listener = ExecutionEvents()\n",
        "        def vd(ep, loss, **kw):\n",
        "            data[\"val_losses\"].append(loss)\n",
        "            data[\"val_acc\"].append(kw[\"acc_all\"])\n",
        "        def ee(ep, loss, avg, **kw):\n",
        "            data[\"losses\"].append(loss)\n",
        "            data[\"acc\"].append(kw[\"acc_all\"])\n",
        "        listener.attach_validated(vd)\n",
        "        listener.attach_epoch_end(ee)\n",
        "        executor.train(cifar_train, 75, train_batch=400, val_batch=600,\n",
        "                        early_stopping=True, patience=6, disturb=False,\n",
        "                        event_listener=listener, metrics={\"acc\":accuracy})\n",
        "        res, data[\"info\"] = executor.eval(cifar_test, batch_size=600, metrics={\"acc\":accuracy})\n",
        "        recording.append(data)\n",
        "        torch.save(recording, \"/content/drive/MyDrive/results-cifar-new.data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OXnagw-2U0p"
      },
      "source": [
        "import torch\n",
        "new_d = torch.load('/content/drive/MyDrive/results-cifar-new.data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNMel-kr2aiR"
      },
      "source": [
        "for i in new_d:\n",
        "    print(\"loss: %s | opt: %s\" % (i[\"loss_reg\"], i[\"opt\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0S6U9362bmO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame()\n",
        "loss_n = []\n",
        "opt_n = []\n",
        "lr = []\n",
        "loss = []\n",
        "acc = []\n",
        "vloss = []\n",
        "vacc = []\n",
        "loss_l = []\n",
        "acc_l = []\n",
        "vloss_l = []\n",
        "vacc_l = []\n",
        "for da in new_d:\n",
        "    loss_n.append(da[\"loss_reg\"])\n",
        "    od = da[\"opt\"].split(\"-\")\n",
        "    opt_n.append(od[0])\n",
        "    lr.append(float(od[1]))\n",
        "    loss.append(np.mean(da[\"losses\"]))\n",
        "    acc.append(np.mean(da[\"acc\"]))\n",
        "    vloss.append(np.mean(da[\"val_losses\"]))\n",
        "    vacc.append(np.mean(da[\"val_acc\"]))\n",
        "    loss_l.append(da[\"losses\"])\n",
        "    acc_l.append(da[\"acc\"])\n",
        "    vloss_l.append(da[\"val_losses\"])\n",
        "    vacc_l.append(da[\"val_acc\"])\n",
        "df[\"loss_name\"] = loss_n\n",
        "df[\"optimizer\"] = opt_n\n",
        "df[\"learning_rate\"] = lr\n",
        "df[\"loss\"] = loss\n",
        "df[\"acc\"] = acc\n",
        "df[\"val_loss\"] = vloss\n",
        "df[\"val_acc\"] = vacc\n",
        "df[\"loss_l\"] = loss_l\n",
        "df[\"acc_l\"] = acc_l\n",
        "df[\"val_loss_l\"] = vloss_l\n",
        "df[\"val_acc_l\"] = vacc_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifS7t8Y22cwb"
      },
      "source": [
        "view_cols = list(filter(lambda t: not t.endswith(\"_l\"), list(df.columns)))\n",
        "df.sort_values(\"val_acc\", ascending=False)[view_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Hmt3mx2eUZ"
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCob1rD92fbx"
      },
      "source": [
        "colorPal = sns.color_palette(palette=[\"#F75590\", \"#3DB1F5\", \"#9EE493\", \"#FFF689\", \"#FF3D3D\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaTcVWRW2gYN"
      },
      "source": [
        "def plot_loss_feat(target, loss, feats, means, titles, pal=None):\n",
        "    fig, axs = plt.subplots(nrows=len(feats))\n",
        "    fig.set_size_inches(10, 20)\n",
        "\n",
        "    for k in range(len(feats)):\n",
        "        x = []\n",
        "        y = []\n",
        "        style = []\n",
        "        df = target\n",
        "        target = df[df[\"loss_name\"] == loss]\n",
        "        for i in range(len(target)):\n",
        "            lab = []\n",
        "            data = target.iloc[i]\n",
        "            v = data[feats[k]]\n",
        "            v = np.array(v)\n",
        "            if means[k]:\n",
        "                v = np.mean(v, axis=1)\n",
        "            x += list(range(len(v)))\n",
        "            y += list(v)\n",
        "            lab.append(data[\"optimizer\"])\n",
        "            style += [\" \".join(lab) for j in v]\n",
        "        if pal is None:\n",
        "            pal = colorPal\n",
        "        fg = sns.lineplot(x=x, y=y, hue=style, ax=axs[k], palette=colorPal)\n",
        "        tt = fg.set_title(titles[k])\n",
        "        plt.setp(tt, color='white')\n",
        "        legend = fg.get_legend()\n",
        "        frame = legend.get_frame()\n",
        "        frame.set_facecolor('#202446')\n",
        "        frame.set_edgecolor('#2D3262')\n",
        "        for text in legend.get_texts():\n",
        "            text.set_color(\"white\")\n",
        "        for idx, ax in enumerate(axs):\n",
        "            ax.spines['bottom'].set_color('white')\n",
        "            ax.spines['top'].set_color('white') \n",
        "            ax.spines['right'].set_color('white')\n",
        "            ax.spines['left'].set_color('white')\n",
        "            ax.xaxis.label.set_color('white')\n",
        "            ax.yaxis.label.set_color('white')\n",
        "            ax.tick_params(colors='white', which='both')\n",
        "            ax.set_facecolor(\"#0e101f\")\n",
        "        fig.set_facecolor(\"#0e101f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFgw-TG2hpM"
      },
      "source": [
        "plot_loss_feat(df[df.learning_rate == 0.001], \"normal\",\n",
        "                   [\"loss_l\", \"val_loss_l\", \"acc_l\", \"val_acc_l\"],\n",
        "                   [True, False, True, True],\n",
        "                   [\"Loss\", \"Validation Loss\", \"Accuracy\", \"Validation Accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ5DohSc2iuO"
      },
      "source": [
        "plot_loss_feat(df[df.learning_rate == 0.01], \"normal\",\n",
        "                   [\"loss_l\", \"val_loss_l\", \"acc_l\", \"val_acc_l\"],\n",
        "                   [True, False, True, True],\n",
        "                   [\"Loss\", \"Validation Loss\", \"Accuracy\", \"Validation Accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_pG9M3n2j_O"
      },
      "source": [
        "ndf = df.copy()\n",
        "ndf[\"optimizer\"] = df[\"optimizer\"] + \"-\" + df[\"learning_rate\"].apply(str)\n",
        "\n",
        "colorPal = sns.color_palette(palette=[\"#F75590\", \"#AF0846\", \"#3DB1F5\", \"#0972AE\", \"#9EE493\", \"#339325\", \"#FFF689\", \"#FFEB0A\", \"#FF3D3D\", \"#A30000\"])\n",
        "plot_loss_feat(ndf, \"normal\",\n",
        "                   [\"loss_l\", \"val_loss_l\", \"acc_l\", \"val_acc_l\"],\n",
        "                   [True, False, True, True],\n",
        "                   [\"Loss\", \"Validation Loss\", \"Accuracy\", \"Validation Accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}